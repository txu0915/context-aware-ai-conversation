{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb9257ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "\n",
    "\n",
    "load_dotenv(dotenv_path=os.path.join('..', '.env'))\n",
    "from prompt_utils import create_evaluation_prompt\n",
    "from llm_utils import evaluate_conversation\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74fda1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 conversation logs to evaluate.\n"
     ]
    }
   ],
   "source": [
    "def load_logs(base_dir='../logs'):\n",
    "    all_conversations = []\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                log_path = os.path.join(root, file)\n",
    "                # Extract the tutor type from the directory path\n",
    "                tutor_type = os.path.basename(root)\n",
    "                with open(log_path, 'r') as f:\n",
    "                    history = json.load(f)\n",
    "                    all_conversations.append({\n",
    "                        'file_path': log_path,\n",
    "                        'tutor_type': tutor_type,\n",
    "                        'history': history\n",
    "                    })\n",
    "    return all_conversations\n",
    "\n",
    "conversation_logs = load_logs()\n",
    "print(f\"Found {len(conversation_logs)} conversation logs to evaluate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b90b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ground truth analysis file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d96952086a24849a09d6a4ea9aaf6d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Conversations:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759435855.615659 70023622 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation complete!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goal_achievement_score</th>\n",
       "      <th>socratic_guidance_score</th>\n",
       "      <th>empathy_and_tone_score</th>\n",
       "      <th>conciseness_and_clarity_score</th>\n",
       "      <th>student_engagement_score</th>\n",
       "      <th>conversational_efficiency_score</th>\n",
       "      <th>justification</th>\n",
       "      <th>tutor_type</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>The tutor demonstrated excellent empathy and t...</td>\n",
       "      <td>non_context_aware</td>\n",
       "      <td>../logs/non_context_aware/conversation_draft_1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>The tutor excelled at guiding the student to i...</td>\n",
       "      <td>non_context_aware</td>\n",
       "      <td>../logs/non_context_aware/conversation_draft_1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>The tutor demonstrated exceptional Socratic gu...</td>\n",
       "      <td>context_aware</td>\n",
       "      <td>../logs/context_aware/conversation_draft_101_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>The tutor excelled at guiding the student to i...</td>\n",
       "      <td>context_aware</td>\n",
       "      <td>../logs/context_aware/conversation_draft_101_2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   goal_achievement_score  socratic_guidance_score  empathy_and_tone_score  \\\n",
       "0                       3                        4                       5   \n",
       "1                       5                        5                       5   \n",
       "2                       3                        5                       5   \n",
       "3                       5                        5                       5   \n",
       "\n",
       "   conciseness_and_clarity_score  student_engagement_score  \\\n",
       "0                              5                         4   \n",
       "1                              4                         5   \n",
       "2                              5                         4   \n",
       "3                              4                         4   \n",
       "\n",
       "   conversational_efficiency_score  \\\n",
       "0                                3   \n",
       "1                                5   \n",
       "2                                5   \n",
       "3                                5   \n",
       "\n",
       "                                       justification         tutor_type  \\\n",
       "0  The tutor demonstrated excellent empathy and t...  non_context_aware   \n",
       "1  The tutor excelled at guiding the student to i...  non_context_aware   \n",
       "2  The tutor demonstrated exceptional Socratic gu...      context_aware   \n",
       "3  The tutor excelled at guiding the student to i...      context_aware   \n",
       "\n",
       "                                           file_path  \n",
       "0  ../logs/non_context_aware/conversation_draft_1...  \n",
       "1  ../logs/non_context_aware/conversation_draft_1...  \n",
       "2  ../logs/context_aware/conversation_draft_101_2...  \n",
       "3  ../logs/context_aware/conversation_draft_101_2...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load the ground truth analysis data ---\n",
    "analysis_path = '../results/student_drafts_with_analysis.csv'\n",
    "try:\n",
    "    df_analysis = pd.read_csv(analysis_path)\n",
    "    # Set draft_id as the index for easy lookup\n",
    "    df_analysis.set_index('draft_id', inplace=True)\n",
    "    print(\"Loaded ground truth analysis file.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Ground truth file not found at {analysis_path}. Please run Notebook 01.\")\n",
    "    df_analysis = None\n",
    "\n",
    "# --- Run the Evaluation ---\n",
    "evaluation_results = []\n",
    "\n",
    "if df_analysis is not None:\n",
    "    for log in tqdm(conversation_logs, desc=\"Evaluating Conversations\"):\n",
    "        try:\n",
    "            # Extract draft_id from the filename (e.g., conversation_draft_101_....json)\n",
    "            draft_id = int(log['file_path'].split('/')[-1].split('_')[2])\n",
    "            \n",
    "            # Fetch the ground truth for this specific conversation\n",
    "            ground_truth = df_analysis.loc[draft_id][['ai_error_type', 'ai_detailed_explanation']].to_dict()\n",
    "            \n",
    "            # Call the updated evaluation function with the ground truth\n",
    "            evaluation = evaluate_conversation(log['history'], ground_truth)\n",
    "            \n",
    "            if evaluation:\n",
    "                result = evaluation.model_dump()\n",
    "                result['tutor_type'] = log['tutor_type']\n",
    "                result['file_path'] = log['file_path']\n",
    "                evaluation_results.append(result)\n",
    "            else:\n",
    "                print(f\"Failed to evaluate {log['file_path']}\")\n",
    "        except (KeyError, IndexError, ValueError):\n",
    "            print(f\"Could not parse draft_id or find ground truth for {log['file_path']}\")\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "eval_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "print(\"\\nEvaluation complete!\")\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb7385e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Average Scores Comparison ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_cf476_row0_col0, #T_cf476_row1_col0, #T_cf476_row1_col4, #T_cf476_row1_col5 {\n",
       "  background-color: #7e9bca;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_cf476_row0_col1, #T_cf476_row0_col2, #T_cf476_row0_col4, #T_cf476_row1_col2 {\n",
       "  background-color: #0d47a1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_cf476_row0_col3, #T_cf476_row0_col5, #T_cf476_row1_col1, #T_cf476_row1_col3 {\n",
       "  background-color: #f0f1f3;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_cf476\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cf476_level0_col0\" class=\"col_heading level0 col0\" >Conciseness And Clarity</th>\n",
       "      <th id=\"T_cf476_level0_col1\" class=\"col_heading level0 col1\" >Conversational Efficiency</th>\n",
       "      <th id=\"T_cf476_level0_col2\" class=\"col_heading level0 col2\" >Empathy And Tone</th>\n",
       "      <th id=\"T_cf476_level0_col3\" class=\"col_heading level0 col3\" >Goal Achievement</th>\n",
       "      <th id=\"T_cf476_level0_col4\" class=\"col_heading level0 col4\" >Socratic Guidance</th>\n",
       "      <th id=\"T_cf476_level0_col5\" class=\"col_heading level0 col5\" >Student Engagement</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >tutor_type</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cf476_level0_row0\" class=\"row_heading level0 row0\" >context_aware</th>\n",
       "      <td id=\"T_cf476_row0_col0\" class=\"data row0 col0\" >4.50</td>\n",
       "      <td id=\"T_cf476_row0_col1\" class=\"data row0 col1\" >5.00</td>\n",
       "      <td id=\"T_cf476_row0_col2\" class=\"data row0 col2\" >5.00</td>\n",
       "      <td id=\"T_cf476_row0_col3\" class=\"data row0 col3\" >4.00</td>\n",
       "      <td id=\"T_cf476_row0_col4\" class=\"data row0 col4\" >5.00</td>\n",
       "      <td id=\"T_cf476_row0_col5\" class=\"data row0 col5\" >4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cf476_level0_row1\" class=\"row_heading level0 row1\" >non_context_aware</th>\n",
       "      <td id=\"T_cf476_row1_col0\" class=\"data row1 col0\" >4.50</td>\n",
       "      <td id=\"T_cf476_row1_col1\" class=\"data row1 col1\" >4.00</td>\n",
       "      <td id=\"T_cf476_row1_col2\" class=\"data row1 col2\" >5.00</td>\n",
       "      <td id=\"T_cf476_row1_col3\" class=\"data row1 col3\" >4.00</td>\n",
       "      <td id=\"T_cf476_row1_col4\" class=\"data row1 col4\" >4.50</td>\n",
       "      <td id=\"T_cf476_row1_col5\" class=\"data row1 col5\" >4.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16346f380>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Justifications for Each Conversation ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tutor_type</th>\n",
       "      <th>justification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>non_context_aware</td>\n",
       "      <td>The tutor demonstrated excellent empathy and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non_context_aware</td>\n",
       "      <td>The tutor excelled at guiding the student to i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>context_aware</td>\n",
       "      <td>The tutor demonstrated exceptional Socratic gu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>context_aware</td>\n",
       "      <td>The tutor excelled at guiding the student to i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tutor_type                                      justification\n",
       "0  non_context_aware  The tutor demonstrated excellent empathy and t...\n",
       "1  non_context_aware  The tutor excelled at guiding the student to i...\n",
       "2      context_aware  The tutor demonstrated exceptional Socratic gu...\n",
       "3      context_aware  The tutor excelled at guiding the student to i..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not eval_df.empty:\n",
    "    # Select only the score columns for the heatmap\n",
    "    score_columns = [col for col in eval_df.columns if col.endswith('_score')]\n",
    "    \n",
    "    # Use pivot_table to average scores if there are multiple logs per tutor type\n",
    "    comparison_df = eval_df.pivot_table(index='tutor_type', values=score_columns, aggfunc='mean')\n",
    "    \n",
    "    # Clean up column names for better display\n",
    "    comparison_df.columns = [col.replace('_score', '').replace('_', ' ').title() for col in comparison_df.columns]\n",
    "    \n",
    "    # Define a modern, high-contrast colormap\n",
    "    modern_blue_cmap = sns.light_palette(\"#0d47a1\", as_cmap=True)\n",
    "\n",
    "    print(\"--- Average Scores Comparison ---\")\n",
    "    styled_df = comparison_df.style.background_gradient(cmap=modern_blue_cmap, axis=None).format(\"{:.2f}\")\n",
    "    display(styled_df)\n",
    "    \n",
    "    # Display the qualitative justifications as well\n",
    "    print(\"\\n--- Justifications for Each Conversation ---\")\n",
    "    justifications_df = eval_df[['tutor_type', 'justification']].reset_index(drop=True)\n",
    "    display(justifications_df)\n",
    "else:\n",
    "    print(\"No evaluation results to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdefc15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tutor_type': {2: 'context_aware', 3: 'context_aware'},\n",
       " 'justification': {2: \"The tutor demonstrated exceptional Socratic guidance, empathy, conciseness, and efficiency. They meticulously guided the student digit-by-digit through the multiplication where the error occurred, without giving any direct answers. The tone was consistently supportive and encouraging, and the questions were laser-focused on leading the student to their specific mistake in the hundreds column of the partial product. While the conversation log ends just before the student reaches the 'aha!' moment and fully corrects the error, the tutor's last prompt perfectly sets up this discovery in the very next turn, showing a deep understanding of the ground truth error and an efficient path to its correction. The student remained engaged throughout the process.\",\n",
       "  3: \"The tutor excelled at guiding the student to identify and correct the specific calculation error detailed in the ground truth context (forgetting to add the carried '1' in the multiplication's hundreds place). The Socratic guidance was exceptionally targeted, progressively narrowing down to the exact point of error with highly relevant questions. The tutor's tone was consistently patient, encouraging, and supportive. The responses were clear and easy to understand, though occasionally a touch verbose with positive affirmations, which slightly impacted the conciseness score. The student showed strong engagement and had a clear 'aha!' moment regarding the identified error. The AI demonstrated a deep understanding of the error type and addressed it very efficiently within the Socratic framework, leading to a swift resolution of the core mistake.\"}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "justifications_df[justifications_df['tutor_type'] == 'context_aware'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dbdd183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tutor_type': {0: 'non_context_aware', 1: 'non_context_aware'},\n",
       " 'justification': {0: \"The tutor demonstrated excellent empathy and tone throughout the conversation, maintaining a supportive and patient demeanor. The responses were consistently clear and concise. The initial guidance was appropriately Socratic, asking the student to explain their steps. The tutor successfully elicited the crucial incorrect multiplication product (3380) from the student, which directly relates to the ground truth error. However, after the student provided '3380?', the tutor took an additional turn asking about the 'reduction' (subtraction) before explicitly guiding the student to double-check the multiplication. While the final turn correctly targets the ground truth error, this slight detour reduced the conversational efficiency and delayed the direct focus on the identified mistake. The student was engaged throughout, but the 'aha!' moment and the full correction of the error had not yet occurred within the provided log.\",\n",
       "  1: \"The tutor excelled at guiding the student to identify and correct the precise calculation error defined in the ground truth. After an initial broad inquiry, the tutor quickly narrowed down to the incorrect multiplication of 145 by 24, and then brilliantly pinpointed the specific partial product error where the student calculated `145 * 4` as `480` instead of `580`. The Socratic questions were highly targeted, leading the student to break down `145 * 4` and explicitly state an 'aha!' moment upon discovering their mistake in summing the sub-products (400 + 160 + 20 = 580). The tutor maintained an consistently empathetic, patient, and encouraging tone throughout. The student remained highly engaged, even through a minor re-direction, culminating in the correct calculation. The overall conversation was efficient in diagnosing and resolving the core error, though some tutor responses were slightly more verbose than strictly necessary, without impeding clarity.\"}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "justifications_df[justifications_df['tutor_type'] == 'non_context_aware'].to_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
